{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "970be063",
   "metadata": {},
   "source": [
    "LLM chatbot has already four steps\n",
    "\n",
    "Load + clean PDF\n",
    "\n",
    "Split into chunks\n",
    "\n",
    "Embed + retrieve\n",
    "\n",
    "RAG prompt → LLM → answer\n",
    "\n",
    "In LangGraph, each of these becomes a node that reads/writes shared state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9badbd3a",
   "metadata": {},
   "source": [
    "Define the graph state\n",
    "\n",
    "LangGraph uses a typed state dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e935602",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, List\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "class RAGState(TypedDict):\n",
    "    question: str\n",
    "    docs: List[Document]\n",
    "    context: str\n",
    "    answer: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a5403e",
   "metadata": {},
   "source": [
    "### Nodes\n",
    "\n",
    "\n",
    "1️⃣ Load + clean PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5e0b648",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_ollama import OllamaEmbeddings, ChatOllama\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "def load_pdf(state: RAGState):\n",
    "    loader = PyPDFLoader(\"1._Intro_to_AI_-_Course_notes.pdf\")\n",
    "    docs = loader.load()\n",
    "\n",
    "    for d in docs:\n",
    "        d.page_content = d.page_content.replace(\"\\x00\", \"\").replace(\"\\n\", \" \").strip()\n",
    "\n",
    "    return {\"docs\": docs}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d13a2a0",
   "metadata": {},
   "source": [
    "2️⃣ Split documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f72a3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_docs(state: RAGState):\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "    chunks = splitter.split_documents(state[\"docs\"])\n",
    "    return {\"docs\": chunks}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482be8f6",
   "metadata": {},
   "source": [
    "3️⃣ Retrieve relevant context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "46e17052",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "\n",
    "vectorstore = Chroma(\n",
    "    persist_directory=\"intro-to-ai\",\n",
    "    embedding_function=embeddings\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 8})\n",
    "\n",
    "\n",
    "def retrieve(state: RAGState):\n",
    "    docs = retriever.invoke(state[\"question\"])\n",
    "    context = \"\\n\\n\".join(d.page_content for d in docs)\n",
    "    return {\"context\": context}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abb7513",
   "metadata": {},
   "source": [
    "4️⃣ Generate answer (RAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a1e0d5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are a helpful assistant.\n",
    "Use the context below to answer the question.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\"\"\")\n",
    "\n",
    "llm = ChatOllama(model=\"llama3\")\n",
    "\n",
    "def generate_answer(state: RAGState):\n",
    "    messages = prompt.invoke({\n",
    "        \"context\": state[\"context\"],\n",
    "        \"question\": state[\"question\"]\n",
    "    })\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"answer\": response.content}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcb7318",
   "metadata": {},
   "source": [
    "### Build the LangGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "90f79693",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "graph = StateGraph(RAGState)\n",
    "\n",
    "graph.add_node(\"load_pdf\", load_pdf)\n",
    "graph.add_node(\"split_docs\", split_docs)\n",
    "graph.add_node(\"retrieve\", retrieve)\n",
    "graph.add_node(\"generate_answer\", generate_answer)\n",
    "\n",
    "graph.set_entry_point(\"load_pdf\")\n",
    "\n",
    "graph.add_edge(\"load_pdf\", \"split_docs\")\n",
    "graph.add_edge(\"split_docs\", \"retrieve\")\n",
    "graph.add_edge(\"retrieve\", \"generate_answer\")\n",
    "graph.add_edge(\"generate_answer\", END)\n",
    "\n",
    "app = graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5c60cfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_chatbot(question: str) -> str:\n",
    "    result = app.invoke({\"question\": question})\n",
    "    return result[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c9e544",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ask_chatbot(\"What did Alan Turing do?\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-chatbot-py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
